{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf600
{\fonttbl\f0\fnil\fcharset0 HelveticaNeue;}
{\colortbl;\red255\green255\blue255;\red27\green31\blue34;\red255\green255\blue255;\red10\green77\blue204;
}
{\*\expandedcolortbl;;\cssrgb\c14118\c16078\c18039;\cssrgb\c100000\c100000\c100000;\cssrgb\c1176\c40000\c83922;
}
\paperw11900\paperh16840\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sl440\sa320\partightenfactor0

\f0\b\fs36 \cf2 \cb3 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Dataset descriptions\
\pard\pardeftab720\sl300\partightenfactor0

\fs30 \cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl360\sa320\partightenfactor0
\cf2 \cb3 \strokec2 Frontal01\
\pard\pardeftab720\sl360\sa320\partightenfactor0

\b0\fs32 \cf2 Frontal01 contains\'a0
\b 70 labeled frontal faces
\b0 \'a0and the original RGB images. Original faces are mainly taken from the MIT-CBCL [3] and FEI [4] datasets.\
\pard\pardeftab720\sl360\sa320\partightenfactor0

\b \cf2 This is the dataset we used in our work [1]
\b0 . Images are organized in two folders - train and test - matching the division we adopted in the paper.\
\pard\pardeftab720\sl300\partightenfactor0

\b\fs30 \cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl360\sa320\partightenfactor0
\cf2 \cb3 \strokec2 Frontal02\
\pard\pardeftab720\sl360\sa320\partightenfactor0

\b0\fs32 \cf2 Frontal02 is an\'a0
\i "high-precision 01"
\i0 . It contains the same images as Frontal01 but with much more precise segmentations.\
\pard\pardeftab720\sl300\partightenfactor0

\b\fs30 \cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl360\sa320\partightenfactor0
\cf2 \cb3 \strokec2 Multipose01\
\pard\pardeftab720\sl360\sa320\partightenfactor0

\b0\fs32 \cf2 Multipose01 contains more than\'a0
\b 200 labeled faces in multiple poses
\b0 . Original faces are taken from the Pointing04 database [5].\
\pard\pardeftab720\sl360\sa320\partightenfactor0

\b \cf2 This is the dataset we used in our work [2]
\b0 . Images are organized in two folders - train and test - matching the division we adopted in the paper.\
\pard\pardeftab720\sl360\sa320\partightenfactor0

\i \cf2 Our advice is: if you need to compare with our results in [1], choose Frontal01. If you need a dataset to train a frontal face segmenter, choose Frontal02. If you are working with multiple head poses, choose Multipose01.
\i0 \
\pard\pardeftab720\sl360\partightenfactor0

\b\fs36 \cf4 \cb1 \strokec4 \
\pard\pardeftab720\sl440\sa320\partightenfactor0
\cf2 \cb3 \strokec2 References\
\pard\pardeftab720\sl360\sa320\partightenfactor0

\b0\fs32 \cf2 [1]\'a0
\i Khalil Khan
\i0 ,\'a0
\i Massimo Mauro
\i0 ,\'a0
\i Riccardo Leonardi
\i0 ,\'a0
\b "Multi-class semantic segmentation of faces"
\b0 , IEEE International Conference on Image Processing (ICIP), 2015 --\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/massimomauro/FASSEG-repository/blob/master/papers/multiclass_face_segmentation_ICIP2015.pdf"}}{\fldrslt 
\b \cf4 \strokec4 PDF}}\
[2]\'a0
\i Khalil Khan
\i0 ,\'a0
\i Massimo Mauro
\i0 ,\'a0
\i Pierangelo Migliorati
\i0 ,\'a0
\i Riccardo Leonardi
\i0 ,\'a0
\b "Head pose estimation through multiclass face segmentation"
\b0 , IEEE International Conference on Multimedia and Expo (ICME), 2017\'a0
\i In collaboration with\'a0{\field{\*\fldinst{HYPERLINK "http://www.yonderlabs.com/"}}{\fldrslt \cf4 \strokec4 YonderLabs}}
\i0 \'a0--\'a0{\field{\*\fldinst{HYPERLINK "https://github.com/massimomauro/FASSEG-repository/blob/master/papers/pose_estimation_by_segmentation_ICME2017.pdf"}}{\fldrslt 
\b \cf4 \strokec4 PDF}}\
[3]\'a0
\i MIT Center for Biological and Computational Learning (CBCL)
\i0 ,\'a0
\b MIT-CBCL database
\b0 ,\'a0{\field{\*\fldinst{HYPERLINK "http://cbcl.mit.edu/software-datasets/FaceData2.html"}}{\fldrslt \cf4 \strokec4 http://cbcl.mit.edu/software-datasets/FaceData2.html}}\
[4]\'a0
\i Centro Universitario da FEI
\i0 ,\'a0
\b FEI database
\b0 ,\'a0{\field{\*\fldinst{HYPERLINK "http://www.fei.edu.br/~cet/facedatabase.html"}}{\fldrslt \cf4 \strokec4 http://www.fei.edu.br/~cet/facedatabase.html}}\
\pard\pardeftab720\sl360\partightenfactor0
\cf2 [5]\'a0
\i Nicolas Gourier, Daniela Hall, and James L Crowley
\i0 ,\'a0
\b \'93Estimating face orientation from robust detection of salient facial structures\'94
\b0 \'a0in FG Net Workshop on Visual Observation of Deictic Gestures. FGnet (IST\'96 2000\'9626434) Cambridge, UK, 2004, pp. 1\'969\
}